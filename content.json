{"pages":[],"posts":[{"title":"Linux 使用","text":"git 安装：yum install git 配置用户名：git config –global user.name “Your Name” 密码：git config –global user.email “email@example.com“ 查看配置是否生效：git config –list 创建本地仓库： 123mkdir gitspacecd gitspacegit init 添加文件 git add redme 提交 git commit -m “add readme” 安装nodejs 下载node安装包到指定目录 https://nodejs.org/en/download/ wget https://npm.taobao.org/mirrors/node/v11.0.0/node-v11.0.0.tar.gz解压安装包 tar -xvf node-v11.0.0.tar.gz进入目录并安装相关插件 cd node-v11.0.0sudo yum install gcc gcc-c++ 进行默认配置并编译 ./configuremake编译大概需要半小时左右～ 之后就可以开始安装 sudo make install之后验证安装 node -v就会发现已经安装完成 vim使用 保存文件 按ESC键 跳到命令模式，然后输入： :w - 保存文件，不退出 vim:w file -将修改另外保存到 file 中，不退出 vim:w! -强制保存，不退出 vim:wq -保存文件，退出 vim:wq! -强制保存文件，退出 vim:q -不保存文件，退出 vim:q! -不保存文件，强制退出 vim:e! -放弃所有修改，从上次保存文件开始再编辑 看某个端口是否打开 lsof -i :80(端口号) alias 命令别名设定：alias lm = ‘ls - al’ type 查询命令是否为bash 的内建命令：type cd echo 取变量：echo $HOME last：显示登录者信息 cut：截取某一行 grep：截取关键字：last | grep ‘root’ cat：输出文件内容：cat /etc/passwd | sort more：一页一页查看，只能向后翻页 less：一页一页查看，可以前后翻页 head 输出前面几行：head -n 20 tail 输出后面几行 sort：排序 uniq：重复内容只显示一个 -c 进行计数 last | cut -d ‘ ’ -f1 | sort | uniq -c wc 显示文档的行数 字数 字节数 -l：仅列出行 -w：仅列出字数 -m：仅列出字节数 文件的压缩与解压 压缩格式：zip，gzip，bzip2，xz 打包格式：tar gzip（.gz） 当你使用gzip进行压缩时，在预设的状态下原本的档案会被压缩成为.gz的档名，原始档案就不再存在了。 -d：解压缩 -c：压缩 gzip -d example.gz bzip2（压缩比比gzip要好） 用法与gzip相同 -k：保留原来的文件不删除 xz（用法类似，压缩比更高，当然更费时） tar（打包） -c：打包 -v：压缩/解压过程中显示正在处理的文件名 -t：查看打包档案中都有哪些文件 -x：解包或解压缩 -j：通过bzip2进行压缩/解压 -J：通过xz进行压缩/解压 -z：通过gzip进行压缩/解压 -f：写在最后，后面接打包压缩文件名 -C：后面接解压缩到的特定目录 例子：压缩，tar -jcvf filename.tar.bz2 查看：tar -jtvf filename.tar.bz2 解压：tar -jxvf filename.tar.bz2 目录操作 rmdir：删除空文件夹 cp：复制文件或文件夹 rm：删除 -f：force，强制删除 -i：删除前询问 -r：递归删除，（文件夹） touch 新建文档","link":"/2020/09/10/Linux%20%E4%BD%BF%E7%94%A8/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2020/10/30/hello-world/"},{"title":"Neo4j 使用说明","text":"1. 简介 没有SQL 图形数据库 GDBMS 关系检索快 表示节点，关系和属性中的数据 节点和关系都包含属性 关系连接节点 属性是键值对 节点用圆圈表示，关系用方向键表示。 关系具有方向：单向和双向。 2. 构成Neo4j图数据库主要有以下构建块 节点 属性 关系 标签 数据浏览器 节点 节点是图表的基本单位。 它包含具有键值对的属性。如下图： 属性 键值对，用来描述图像、关系。 关系 表示两个节点之间的关系。有向。 标签 Label将一个公共名称与一组节点或关系相关联。 节点或关系可以包含一个或多个标签。 我们可以为现有节点或关系创建新标签。 我们可以从现有节点或关系中删除现有标签。 **注： **Neo4j将数据存储在节点或关系的属性中。 Neo4j 数据浏览器 neo4j start http://localhost:7474/browser/ 3. Cypher 查询语言 关键词 CQL 用法 CREATE 创建节点、关系和属性 MATCH 检索有关节点、关系和属性 RETURN 返回查询的结果 WHERE 查询条件 DELETE 删除节点、关系 REMOVE 删除节点、关系的属性 ORDER BY 排序 SET 添加或更新标签 数据类型 数据类型 说明 boolean True，False byte 用于表示8位整数 short 用于表示16位整数 int 用于表示32位整数 long 用于表示64位整数 float 用于表示32位浮点数 double 用于表示64位浮点数 char 用于表示16位字符 string 用于表示字符串 CREATE用于创建节点。 1234# 单个标签CREATE (&lt;node-name&gt;:&lt;label-name&gt;)# 多个标签CREATE (&lt;node-name&gt;:&lt;label-name1&gt;:&lt;label-name2&gt;.....:&lt;label-namen&gt;) : 节点名称 : 标签名称 创建时可以带有属性： 12345678CREATE ( &lt;node-name&gt;:&lt;label-name&gt; { &lt;Property1-name&gt;:&lt;Property1-Value&gt; ........ &lt;Propertyn-name&gt;:&lt;Propertyn-Value&gt; }) 例如： 1CREATE (dept:Dept { deptno:10,dname:&quot;Accounting&quot;,location:&quot;Hyderabad&quot; }) 注意：属性以字典形式声明，属性名不用加引号，属性值数字不加引号，字符加引号。属性间用逗号间隔，属性、标签和属性值之间不用逗号隔开。 MATCH1MATCH (&lt;node-name&gt;:&lt;label-name&gt;) : 节点名称 : 标签名称 例如： 1MATCH (e: Employee) RETURN e 注意：我们不能单独使用MATCH Command从数据库检索数据。 ​ MATCH (e: ‘Employee’) RETURN e ​ MATCH (e: “Employee”) RETURN e ​ MATCH (e: Employee) RETURN e 三个命令相同，我们可以选择这些命令中的任何一个。 RETURN用于检索节点、关系的属性 1234RETURN &lt;node-name&gt;.&lt;property1-name&gt;, ........ &lt;node-name&gt;.&lt;propertyn-name&gt; 例如： 1RETURN dept.deptno 注意：同样，不能单独使用RETURN语句返回属性 MATCH &amp; RETURNmatch 和 return 联合使用才能完成查询： 12MATCH (dept: Dept)RETURN dept.deptno,dept.dname 查询节点所有属性： 12MATCH (dept: Dept)RETURN dept 关系查询 创建节点之间的关系 123CREATE (&lt;node1-name&gt;:&lt;label1-name&gt;)- [(&lt;relationship-name&gt;:&lt;relationship-label-name&gt;)] -&gt;(&lt;node2-name&gt;:&lt;label2-name&gt;) 例如： 1CREATE (p1:Profile1)-[r1:LIKES]-&gt;(p2:Profile2) WHERE简单条件 1WHERE &lt;condition&gt; 复合条件 1WHERE &lt;condition&gt; &lt;boolean-operator&gt; &lt;condition&gt; 用法 1&lt;property-name&gt; &lt;comparison-operator&gt; &lt;value&gt; 其中，比较运算符有： 运算符 说明 = 等于 &lt;&gt; 不等于 &lt; 小于 &gt; 大于 &lt;= 小于等于 &gt;= 大于等于 条件之间的布尔操作有： 布尔操作 说明 AND 与 OR 或 NOT 非 XOR 异或 例如： 123MATCH (emp:Employee) WHERE emp.name = 'Abc' OR emp.name = 'Xyz'RETURN emp 创建关系例如： 1234MATCH (cust:Customer),(cc:CreditCard) WHERE cust.id = &quot;1001&quot; AND cc.id= &quot;5001&quot; CREATE (cust)-[r:DO_SHOPPING_WITH{shopdate:&quot;12/12/2014&quot;,price:55000}]-&gt;(cc) RETURN r DELETE用于删除节点及其关联的属性 1DELETE &lt;node-name-list&gt; 1DELETE &lt;node1-name&gt;,&lt;node2-name&gt;,&lt;relationship-name&gt; 注意：我们应该使用逗号（，）运算符来分隔节点名。 例如： 删除节点 1MATCH (e: Employee) DELETE e 删除关系 1MATCH (cc: CreditCard)-[rel]-(c:Customer) DELETE cc,c,rel REMOVE用于删除现有节点或关系的属性 Neo4j CQL DELETE和REMOVE命令之间的主要区别： DELETE操作用于删除节点和关联关系。 REMOVE操作用于删除标签和属性。 注：两个命令都应该与MATCH命令一起使用。 删除节点属性： 1REMOVE &lt;property-name-list&gt; 例如： 123MATCH (book { id:122 })REMOVE book.priceRETURN book 等同于： 1234567MATCH (book: Book)WHERE book.id = 122REMOVE book.priceRETURN book 等同于： 12SELECT * FROM BOOK WHERE ID = 122;ALTER TABLE BOOK REMOVE COLUMN PRICE; 删除节点、关系的某个标签 1REMOVE &lt;label-name-list&gt; 例如： 12MATCH (m:Movie) REMOVE m:Picture SET用于为节点、关系添加或更新属性 1SET &lt;node-label-name&gt;.&lt;property1-name&gt; 例如： 123MATCH (book:Book)SET book.title = 'superstar'RETURN book ORDER BY用于查询结果排序，默认是DESC（降序） 例如： 123MATCH (emp:Employee)RETURN emp.empid,emp.name,emp.salary,emp.deptnoORDER BY emp.name DESC UNION &amp; UNION ALLUNION 用于将两组结果中的公共行组合并返回到一组结果中。 123&lt;MATCH Command1&gt; UNION&lt;MATCH Command2&gt; 例如： 123MATCH (cc:CreditCard) RETURN cc.id,cc.numberUNIONMATCH (dc:DebitCard) RETURN dc.id,dc.number 返回包含在两个查询结果中的不重复的行。 UNION ALL 则返回两组结果中的所有行。 LIMIT用来过滤或限制查询返回的行数。 1LIMIT &lt;number&gt; 例如： 123MATCH (emp:Employee) RETURN empLIMIT 2 只显示Top 2 的结果。 对应地，SKIP用于跳过最前面的查询结果： 1SKIP &lt;number&gt; 例如： 123MATCH (emp:Employee) RETURN empSKIP 2 返回从第3个开始的结果。 MERGE用于将新的节点添加到数据库，如果存在则返回该节点；否则创建新节点。 12345678MERGE = CREATE + MATCHMERGE (&lt;node-name&gt;:&lt;label-name&gt;{ &lt;Property1-name&gt;:&lt;Pro&lt;rty1-Value&gt; ..... &lt;Propertyn-name&gt;:&lt;Propertyn-Value&gt;}) 与CREATE的区别： CREATE 在创建新节点时，不检查数据库中是否已存在相同节点；因此会创建重复数据。 NULL空属性用NULL填充 IN类似于 AND 操作： 123MATCH (e:Employee) WHERE e.id IN [123,124]RETURN e.id,e.name,e.sal,e.deptno 4. Neo4j 函数字符串函数 功能 描述 UPPER 所有字母改为大写 LOWER 所有字母转为小写 SUBSTRING 字串 REPLACE 替换 1UPPER (&lt;input-string&gt;) 1SUBSTRING(&lt;input-string&gt;,&lt;startIndex&gt; ,&lt;endIndex&gt;) 例如： 12MATCH (e:Employee) RETURN e.id,UPPER(e.name),LOWER(e.sal),e.deptno 12MATCH (e:Employee) RETURN e.id,SUBSTRING(e.name,0,2),e.sal,e.deptno AGGERGATION 聚合类似于SQL中的GROUP BY子句。 功能 描述 COUNT 返回行数 MAX 返回最大值 MIN 返回最小值 SUM 求和 AVG 平均 例如： 1MATCH (e:Employee) RETURN COUNT(*) 12MATCH (e:Employee) RETURN MAX(e.sal),MIN(e.sal) 12MATCH (e:Employee) RETURN SUM(e.sal),AVG(e.sal) 关系函数STARTNODE、ENDNODE 返回关系的开始节点 1STARTNODE (&lt;relationship-label-name&gt;) 例如： 12MATCH (a)-[movie:ACTION_MOVIES]-&gt;(b) RETURN STARTNODE(movie),ENDNODE(movie) 12MATCH (a)-[movie:ACTION_MOVIES]-&gt;(b) RETURN ID(movie),TYPE(movie) 关系的ID和类型（名称） 5. Neo4j Admin索引 Neo4j SQL支持节点或关系属性上的索引，以提高应用程序的性能。 我们可以为具有相同标签名称的所有节点的属性创建索引。 Create Index 创建索引 Drop Index 丢弃索引 创建索引： 1CREATE INDEX ON :&lt;label_name&gt; (&lt;property_name&gt;) 上述语法描述它在节点或关系的的上创建一个新索引。 删除索引： 1DROP INDEX ON :&lt;label_name&gt; (&lt;property_name&gt;) 例如： 12CREATE INDEX ON :Customer (name)DROP INDEX ON :Customer (name) UNIQUE 创建UNIQUE约束 丢弃UNIQUE约束 12CREATE CONSTRAINT ON (&lt;label_name&gt;)ASSERT &lt;property_name&gt; IS UNIQUE 12CREATE CONSTRAINT ON (cc:CreditCard)ASSERT cc.number IS UNIQUE 创建新的节点时，如果被UNIQUE约束的属性已存在，则会创建失败。 12DROP CONSTRAINT ON (&lt;label_name&gt;)ASSERT &lt;property_name&gt; IS UNIQUE 删除约束","link":"/2020/10/31/Neo4j-%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E/"},{"title":"人工智能领域顶会","text":"第一梯队IJCAIAI最好的综合性会议, 1969年开始, 每两年开一次, 奇数年开. 每届基本上能录100多篇（现在已经到200多篇了），但分到每个领域就没几篇了。 AAAI美国人工智能学会AAAI的年会.比IJCAI还是要稍弱一点 COLT计算学习理论最好的会议, ACM主办, 每年举行. 计算学习理论基本上可以看成理论计算机科学和机器学习的交叉。“一小群数学家在开会” CVPR计算机视觉和模式识别方面最好的会议之一, IEEE主办, 每年举行. 虽然题目上有计算机视觉, 但个人认为它的模式识别味道更重一些. 事实上它应该是模式识别最好的会议, 而在计算机视觉方面, 还有ICCV与之相当. ICCV计算机视觉方面最好的会之一. IEEE主办, 每年举行. ICML机器学习方面最好的会议之一. 现在是IMLS主办, 每年举行. NIPS神经计算方面最好的会议之一, NIPS主办, 每年举行.NIPS里有相当一部分神经科学的内容, 和机器学习有一定的距离. 但由于会议的主体内容是机器学习, 或者说与机器学习关系紧密, 所以 不少人把NIPS看成是机器学习方面最好的会议之一.对Jordan系以外的人来说, 发NIPS的难度比ICML更大. 换句话说, ICML比较开放, 小圈子的影响不象NIPS那么大, 所以北美和欧洲人都认, 而NIPS则有些人(特别是一些欧洲人, 包括一些大家)坚决不投稿.Michael Jordan是伯克利大学教授，统计机器学习的老大，大牛中的巨牛 ACL计算语言学/自然语言处理方面最好的会议, ACL (Association of Computational Linguistics) 主办, 每年开. KR知识表示和推理方面最好的会议之一, 实际上也是传统AI(即基于逻辑的AI) 最好的会议之一. KR Inc.主办, 现在是偶数年开. SIGIR信息检索方面最好的会议, ACM主办, 每年开. 这个会现在小圈子气越来越重. 信息检索应该不算AI, 不过因为这里面用到机器学习越来越多, 最近几年甚至有点机器学习应用会议的味道了 SIGKDD数据挖掘方面最好的会议, ACM主办, 每年开. ICLRInternational Conference on Learning Representations,深度学习领域最重要的会议之一，尽管才第五届，已经有很多非常重要的文章，比如VGG Net,attention等 第二梯队AAMAS (2+)agent方面最好的会议. 但是现在agent已经是一个一般性的概念, 几乎所有AI有关的会议上都有这方面的内容, 所以AAMAS下降的趋势非常明显. ECCV (2+)计算机视觉方面仅次于ICCV的会议, 因为这个领域发展很快, 有可能升级到1-去. ECML (2+)机器学习方面仅次于ICML的会议, 欧洲人极力捧场,因为机器学习发展很快, 这个会议的reputation上升非常明显. ICDM (2+)数据挖掘方面仅次于SIGKDD的会议, 目前和SDM相当. 这个会只有5年历史, 上升速度之快非常惊人. 几年前ICDM还比不上PAKDD, 现在已经拉开很大距离了. SDM (2+)数据挖掘方面仅次于SIGKDD的会议, 目前和ICDM相当. SIAM的底子很厚, 但在CS里面的影响比ACM和IEEE还是要小, SDM眼看着要被ICDM超过了, 但至少目前还是相当的. COLLING (2)计算语言学/自然语言处理方面仅次于ACL的会, 但与ACL的差距比ICCV-ECCV和ICML-ECML大得多. ECAI (2)欧洲的人工智能综合型会议, 历史很久, 但因为有IJCAI/AAAI压着,很难往上升. EMNLP (2-)计算语言学/自然语言处理方面一个不错的会. 有些人认为与COLLING相当, 但我觉得它还是要弱一点. 第三梯队ACCV (3+)亚洲的计算机视觉会议, 在亚太级别的会议里算很好的了. ICIP (3)图像处理方面最著名的会议之一, 盛会型. ICPR (3)模式识别方面最著名的会议之一, 盛会型. IJNLP (3)计算语言学/自然语言处理方面比较著名的一个会议. IJCNN (3)神经网络方面最重要的会议, 盛会型, 参见CEC的介绍.","link":"/2020/10/20/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E9%A2%86%E5%9F%9F%E9%A1%B6%E4%BC%9A/"},{"title":"BERT 详解","text":"0. NLP 发展史 2001 - Neural language models（神经语言模型） 2008 - Multi-task learning（多任务学习） 2013 - Word embeddings（词嵌入） 2013 - Neural networks for NLP（NLP神经网络） 2014 - Sequence-to-sequence models 2015 - Attention（注意力机制） 2015 - Memory-based networks（基于记忆的网络） 2018 - Pretrained language models（预训练语言模型） 1. NLP 任务分类 SRL (语义角色标注) Coref (指代消歧) SNLI (自然语言推理) SQuAD (阅读理解，问答系统) SST-S (情感分析) **NER **(命名实体识别) 2. Seq2Seq 在encode阶段，第一个节点输入一个词，之后的节点输入的是下一个词与前一个节点的hidden state，最终encoder会输出一个context，这个context又作为decoder的输入，每经过一个decoder的节点就输出一个翻译后的词，并把decoder的hidden state作为下一层的输入。该模型对于短文本的翻译来说效果很好，但是其也存在一定的缺点，如果文本稍长一些，就很容易丢失文本的一些信息，为了解决这个问题，Attention应运而生。 Attention Attention与传统的Seq2Seq模型主要有以下两点不同。 1）encoder提供了更多的数据给到decoder，encoder会把所有的节点的hidden state提供给decoder，而不仅仅只是encoder最后一个节点的hidden state。2）decoder并不是直接把所有encoder提供的hidden state作为输入，而是采取一种选择机制，把最符合当前位置的hidden state选出来，具体的步骤如下 确定哪一个hidden state与当前节点关系最为密切 计算每一个hidden state的分数值（具体怎么计算我们下文讲解） 对每个分数值做一个softmax的计算，这能让相关性高的hidden state的分数值更大，相关性低的hidden state的分数值更低 Attention模型并不只是盲目地将输出的第一个单词与输入的第一个词对齐。实际上，它在训练阶段学习了如何在该语言对中对齐单词(示例中是法语和英语)。Attention函数的本质可以被描述为一个查询（query）到一系列（键key-值value）对的映射。 3. Transformer《Attention Is All You Need》是一篇Google提出的将Attention思想发挥到极致的论文。这篇论文中提出一个全新的模型，叫 Transformer，抛弃了以往深度学习任务里面使用到的 CNN 和 RNN ，Bert就是基于Transformer构建的，这个模型广泛应用于NLP领域，例如机器翻译，问答系统，文本摘要和语音识别等等方向。关于Transrofmer模型的理解特别推荐一位国外博主文章《The Illustrated Transformer》。 总体结构 和Attention模型一样，Transformer模型中也采用了 encoer-decoder 架构。但其结构相比于Attention更加复杂，论文中encoder层由6个encoder堆叠在一起，decoder层也一样。 每一个encoder和decoder的内部简版结构如下图： Encoder对于encoder，包含两层，一个self-attention层和一个前馈神经网络，self-attention能帮助当前节点不仅仅只关注当前的词，从而能获取到上下文的语义。decoder也包含encoder提到的两层网络，但是在这两层中间还有一层attention层，帮助当前节点获取到当前需要关注的重点内容。 首先，模型需要对输入的数据进行一个embedding操作，（也可以理解为类似w2c的操作），enmbedding结束之后，输入到encoder层，self-attention处理完数据后把数据送给前馈神经网络，前馈神经网络的计算可以并行，得到的输出会输入到下一个encoder。 Self-Attention 1、首先，self-attention会计算出三个新的向量，在论文中，向量的维度是512维，我们把这三个向量分别称为Query、Key、Value，这三个向量是用embedding向量与一个矩阵相乘得到的结果，这个矩阵是随机初始化的，维度为（64，512）注意第二个维度需要和embedding的维度一样，其值在BP的过程中会一直进行更新，得到的这三个向量的维度是64低于embedding维度的。 ​ $q = x⋅W^q$ ​ $k = x⋅W^k$ ​ $v = x⋅W^v$ 2、计算self-attention的分数值，该分数值决定了当我们在某个位置encode一个词时，对输入句子的其他部分的关注程度。这个分数值的计算方法是Query与Key做点乘，以下图为例，首先我们需要针对Thinking这个词，计算出其他词对于该词的一个分数值，首先是针对于自己本身即q1·k1，然后是针对于第二个词即q1·k2. 3、接下来，把点成的结果除以一个常数，这里我们除以8，这个值一般是采用上文提到的矩阵的第一个维度的开方即64的开方8，当然也可以选择其他的值，然后把得到的结果做一个softmax的计算。得到的结果即是每个词对于当前位置的词的相关性大小，当然，当前位置的词相关性肯定会会很大. 4、下一步就是把Value和softmax得到的值进行相乘，并相加，得到的结果即是self-attetion在当前节点的值。 这种通过 query 和 key 的相似性程度来确定 value 的权重分布的方法被称为scaled dot-product attention。其实scaled dot-Product attention就是我们常用的使用点积进行相似度计算的attention，只是多除了一个（为K的维度）起到调节作用，使得内积不至于太大。 对于使用自注意力机制的原因，论文中提到主要从三个方面考虑（每一层的复杂度，是否可以并行，长距离依赖学习），并给出了和RNN，CNN计算复杂度的比较。可以看到，如果输入序列n小于表示维度d的话，每一层的时间复杂度self-attention是比较有优势的。当n比较大时，作者也给出了一种解决方案self-attention（restricted）即每个词不是和所有词计算attention，而是只与限制的r个词去计算attention。在并行方面，多头attention和CNN一样不依赖于前一时刻的计算，可以很好的并行，优于RNN。在长距离依赖上，由于self-attention是每个词和所有词都要计算attention，所以不管他们中间有多长距离，最大的路径长度也都只是1。可以捕获长距离依赖关系。 Positional Encoding 到目前为止，transformer模型中还缺少一种解释输入序列中单词顺序的方法。为了处理这个问题，transformer给encoder层和decoder层的输入添加了一个额外的向量Positional Encoding，维度和embedding的维度一样，这个向量采用了一种很独特的方法来让模型学习到这个值，这个向量能决定当前词的位置，或者说在一个句子中不同的词之间的距离。这个位置向量的具体计算方法有很多种，论文中的计算方法如下其中pos是指当前词在句子中的位置，i是指向量中每个值的index，可以看出，在偶数位置，使用正弦编码，在奇数位置，使用余弦编码。最后把这个Positional Encoding与embedding的值相加，作为输入送到下一层。为了让模型捕捉到单词的顺序信息，我们添加位置编码向量信息（POSITIONAL ENCODING），位置编码向量不需要训练，它有一个规则的产生方式（上图公式）。 如果我们的嵌入维度为4，那么实际上的位置编码就如下图所示： Layer Normalization 在transformer中，每一个子层（self-attetion，ffnn）之后都会接一个残差模块，并且有一个Layer normalization. Normalization有很多种，但是它们都有一个共同的目的，那就是把输入转化成均值为0方差为1的数据。我们在把数据送入激活函数之前进行normalization（归一化），因为我们不希望输入数据落在激活函数的饱和区。 Batch Normalization 的主要思想就是：在每一层的每一批数据上进行归一化。我们可能会对输入数据进行归一化，但是经过该网络层的作用后，我们的数据已经不再是归一化的了。随着这种情况的发展，数据的偏差越来越大，我的反向传播需要考虑到这些大的偏差，这就迫使我们只能使用较小的学习率来防止梯度消失或者梯度爆炸。 BN的具体做法就是对每一小批数据，在批这个方向上做归一化。如下图所示：可以看到，右半边求均值是沿着数据 batch_size的方向进行的，其计算公式如下： 那么Layer normalization 呢？它也是归一化数据的一种方式，不过 LN 是在每一个样本上计算均值和方差，而不是BN那种在批方向计算均值和方差！ 下面看一下 LN 的公式： 在self-attention需要强调的最后一点是其采用了残差网络中的short-cut结构，目的是解决深度学习中的退化问题。 Decoder 可以看到decoder部分其实和encoder部分大同小异，不过在最下面额外多了一个masked mutil-head attetion，这里的mask也是transformer一个很关键的技术，我们一起来看一下。 Mask mask 表示掩码，它对某些值进行掩盖，使其在参数更新时不产生效果。Transformer 模型里面涉及两种 mask，分别是 padding mask 和 sequence mask。其中，padding mask 在所有的 scaled dot-product attention 里面都需要用到，而 sequence mask 只有在 decoder 的 self-attention 里面用到。 Padding mask 什么是 padding mask 呢？因为每个批次输入序列长度是不一样的也就是说，我们要对输入序列进行对齐。具体来说，就是给在较短的序列后面填充 0。但是如果输入的序列太长，则是截取左边的内容，把多余的直接舍弃。因为这些填充的位置，其实是没什么意义的，所以我们的attention机制不应该把注意力放在这些位置上，所以我们需要进行一些处理。 具体的做法是，把这些位置的值加上一个非常大的负数(负无穷)，这样的话，经过 softmax，这些位置的概率就会接近0！ 而我们的 padding mask 实际上是一个张量，每个值都是一个Boolean，值为 false 的地方就是我们要进行处理的地方。 Sequence mask sequence mask 是为了使得 decoder 不能看见未来的信息。也就是对于一个序列，在 time_step 为 t 的时刻，我们的解码输出应该只能依赖于 t 时刻之前的输出，而不能依赖 t 之后的输出。因此我们需要想一个办法，把 t 之后的信息给隐藏起来。 那么具体怎么做呢？也很简单：产生一个上三角矩阵，上三角的值全为0。把这个矩阵作用在每一个序列上，就可以达到我们的目的。 对于 decoder 的 self-attention，里面使用到的 scaled dot-product attention，同时需要padding mask 和 sequence mask 作为 attn_mask，具体实现就是两个mask相加作为attn_mask。 其他情况，attn_mask 一律等于 padding mask。 编码器通过处理输入序列启动。然后将顶部编码器的输出转换为一组注意向量k和v。每个解码器将在其“encoder-decoder attention”层中使用这些注意向量，这有助于解码器将注意力集中在输入序列中的适当位置： 完成编码阶段后，我们开始解码阶段。解码阶段的每个步骤从输出序列（本例中为英语翻译句）输出一个元素。以下步骤重复此过程，一直到达到表示解码器已完成输出的符号。每一步的输出在下一个时间步被送入底部解码器，解码器像就像我们对编码器输入所做操作那样，我们将位置编码嵌入并添加到这些解码器输入中，以表示每个字的位置。 输出层当decoder层全部执行完毕后，怎么把得到的向量映射为我们需要的词呢，很简单，只需要在结尾再添加一个全连接层和softmax层，假如我们的词典是1w个词，那最终softmax会输入1w个词的概率，概率值最大的对应的词就是我们最终的结果。 4. BERT模型结构 L表示的是transformer的层数，H表示输出的维度，A表示mutil-head attention的个数，uncased和cased的区别在于uncased将全部样本变为小写，而cased则要区分大小写。 预训练模型 首先我们要了解一下什么是预训练模型，举个例子，假设我们有大量的维基百科数据，那么我们可以用这部分巨大的数据来训练一个泛化能力很强的模型，当我们需要在特定场景使用时，例如做文本相似度计算，那么，只需要简单的修改一些输出层，再用我们自己的数据进行一个增量训练，对权重进行一个轻微的调整。 预训练的好处在于在特定场景使用时不需要用大量的语料来进行训练，节约时间效率高效，bert就是这样的一个泛化能力较强的预训练模型。 BERT的预训练阶段包括两个任务，一个是Masked Language Model，还有一个是Next Sentence Prediction。 Masked Language Model MLM可以理解为完形填空，作者会随机mask每一个句子中15%的词，用其上下文来做预测，例如：my dog is hairy → my dog is [MASK] 此处将hairy进行了mask处理，然后采用非监督学习的方法预测mask位置的词是什么，但是该方法有一个问题，因为是mask15%的词，其数量已经很高了，这样就会导致某些词在fine-tuning阶段从未见过，为了解决这个问题，作者做了如下的处理： 80%的时间是采用[mask]，my dog is hairy → my dog is [MASK] 10%的时间是随机取一个词来代替mask的词，my dog is hairy -&gt; my dog is apple 10%的时间保持不变，my dog is hairy -&gt; my dog is hairy Next Sentence Prediction 选择一些句子对A与B，其中50%的数据B是A的下一条句子，剩余50%的数据B是语料库中随机选择的，学习其中的相关性，添加这样的预训练的目的是目前很多NLP的任务比如QA和NLI都需要理解两个句子之间的关系，从而能让预训练的模型更好的适应这样的任务。 输入 BERT的输入词向量是三个向量之和： Token Embedding：WordPiece tokenization subword词向量。Segment Embedding：表明这个词属于哪个句子（NSP需要两个句子）。Position Embedding：学习出来的embedding向量。这与Transformer不同，Transformer中是预先设定好的值。 输出 BERT预训练模型的输出结果，无非就是一个或多个向量。下游任务可以通过精调（改变预训练模型参数）或者特征抽取（不改变预训练模型参数，只是把预训练模型的输出作为特征输入到下游任务）两种方式进行使用。BERT原论文使用了精调方式，但也尝试了特征抽取方式的效果，比如在NER任务上，最好的特征抽取方式只比精调差一点点。但特征抽取方式的好处可以预先计算好所需的向量，存下来就可重复使用，极大提升下游任务模型训练的速度。 总结BERT优点 Transformer Encoder因为有Self-attention机制，因此BERT自带双向功能 因为双向功能以及多层Self-attention机制的影响，使得BERT必须使用Cloze版的语言模型Masked-LM来完成token级别的预训练 为了获取比词更高级别的句子级别的语义表征，BERT加入了Next Sentence Prediction来和Masked-LM一起做联合训练 为了适配多任务下的迁移学习，BERT设计了更通用的输入层和输出层 微调成本小 BERT缺点 task1的随机遮挡策略略显粗犷，推荐阅读《Data Nosing As Smoothing In Neural Network Language Models》 [MASK]标记在实际预测中不会出现，训练时用过多[MASK]影响模型表现; 每个batch只有15%的token被预测，所以BERT收敛得比left-to-right模型要慢（它们会预测每个token） BERT对硬件资源的消耗巨大（大模型需要16个tpu，历时四天；更大的模型需要64个tpu，历时四天。","link":"/2020/10/31/BERT-%E8%AF%A6%E8%A7%A3/"}],"tags":[{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"Neo4j","slug":"Neo4j","link":"/tags/Neo4j/"},{"name":"AI","slug":"AI","link":"/tags/AI/"},{"name":"BERT","slug":"BERT","link":"/tags/BERT/"}],"categories":[{"name":"Linux","slug":"Linux","link":"/categories/Linux/"},{"name":"数据库","slug":"数据库","link":"/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"NLP","slug":"NLP","link":"/categories/NLP/"}]}